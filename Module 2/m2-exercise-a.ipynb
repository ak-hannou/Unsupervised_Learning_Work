{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995c96b5",
   "metadata": {},
   "source": [
    "# Module 2: Exercise A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d54bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1554056e-ce51-4d00-a7cd-1e285ca99008",
   "metadata": {},
   "source": [
    "In this exercise, we will apply a PCA model to the breast cancer dataset, reduce the dimensionality of the data, and subsequently fit a logistic regression model using both the original and reduced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1b0b9-79b6-4161-855f-a1826cff9114",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ef2da-7021-4ada-90ba-5755f2e821e5",
   "metadata": {},
   "source": [
    "Let’s begin by importing the data and creating some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638085bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X_bc, y_bc = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X_bc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b0230-b5d1-452d-b822-edb8378fca4a",
   "metadata": {},
   "source": [
    ">__Task 1__\n",
    ">\n",
    ">- Check unique values in `y_bc`\n",
    ">- Plot the count of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2372d3-221c-4bcb-93b2-31bc6c0ec1bc",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c08eb-9f86-46ba-b544-621a8158abd4",
   "metadata": {},
   "source": [
    ">__Task 2__\n",
    ">\n",
    ">Split the data into `X_train`, `X_test`, `y_train`, `y_test` \n",
    ">\n",
    ">- Set a 80(train):20(test) ratio\n",
    ">- Set 156 randomness and use stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f5563-f5df-4b32-98d5-e90f542b441e",
   "metadata": {},
   "source": [
    "### Standardize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c87200",
   "metadata": {
    "tags": []
   },
   "source": [
    ">__Task 3__\n",
    ">\n",
    ">Standardize both train and test sets\n",
    ">\n",
    ">- Fit the scaler to the train set\n",
    ">- Transform both train and test sets using the fitted scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48153f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee6333-f1d8-405b-859f-abc600e5eff5",
   "metadata": {},
   "source": [
    "## PCA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369dfa4b",
   "metadata": {
    "tags": []
   },
   "source": [
    ">__Task 4__\n",
    ">\n",
    ">- Fit a PCA model on the train set\n",
    ">- Transform both train and test sets\n",
    ">- Save the transformed data in DataFrame named `X_train_pca` and `X_test_pca` respectively\n",
    ">- Check out the first 5 rows of the transformed data points in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd91397",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5392289-bc27-4852-a7d0-a1f00cbcf791",
   "metadata": {},
   "source": [
    ">__Task 5__\n",
    ">\n",
    ">- Print the weight of the first feature (column) in the first principal component (PC)\n",
    ">- Save PCs into a DataFrame and print out the first 5 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d56d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74e813-9690-4605-a5a2-3eccd90991ce",
   "metadata": {},
   "source": [
    "### Visualize the Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15baf4c3",
   "metadata": {
    "tags": []
   },
   "source": [
    ">__Task 6__\n",
    ">\n",
    ">- Print the amount of variance explained by PCs\n",
    ">- Print the proportion of the variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf874e8-a63a-4b64-a147-c1d9304cb50d",
   "metadata": {},
   "source": [
    ">__Task 7__\n",
    ">\n",
    ">Create a scree plot to visualize the proportion of the variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5dcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfa3df-b696-4770-8929-2af212a00081",
   "metadata": {},
   "source": [
    ">__Task 8__\n",
    ">\n",
    ">Create a line plot to show the cumulative variability\n",
    ">\n",
    ">How many principal axes (columns) are necessary to explain 90% variability in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb108b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f0b3a-d9ed-4fa5-8668-43c40a2663d9",
   "metadata": {},
   "source": [
    "Let’s plot the first two principal components and check whether the 2D representation effectively captures and distinguishes the variability in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612d3c9",
   "metadata": {},
   "source": [
    ">__Task 9__\n",
    ">\n",
    ">Visualize the first two PCs (Hint: use scatter plot from seaborn and set `hue`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840468e0",
   "metadata": {},
   "source": [
    "From data visualization, we can see that the transformed 2D data can differentiate between class 0 and class 1 to some extent. Let’s proceed with the actual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5afc9f-d3e3-40f1-bad1-15549b7bffba",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b3dca",
   "metadata": {},
   "source": [
    ">__Task 10__\n",
    ">\n",
    ">Implement a logistic regression model on the __original data__\n",
    ">\n",
    ">- Fit a logistic regression model on the train set\n",
    ">- Make predictions on the test set\n",
    ">- Calculate the *accuracy*, *precision* and *recall* using predictions and actual target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db502c",
   "metadata": {},
   "source": [
    ">__Task 11__\n",
    ">\n",
    ">Implement a logistic regression model on the __transformed data by PCA__\n",
    ">\n",
    ">- Fit a logistic regression model on the train set\n",
    ">- Make predictions on the test set\n",
    ">- Calculate the *accuracy*, *precision* and *recall* using predictions and actual target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f082f9d",
   "metadata": {},
   "source": [
    ">__Task 12__\n",
    ">\n",
    ">Implement a logistic regression model on the __first 5 columns of the transformed data by PCA__\n",
    ">\n",
    ">- Fit a logistic regression model on the train set\n",
    ">- Make predictions on the test set\n",
    ">- Calculate the *accuracy*, *precision* and *recall* using predictions and actual target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cae3df",
   "metadata": {},
   "source": [
    ">__Task 13__\n",
    ">\n",
    ">Implement a logistic regression model on the __first 2 columns of the transformed data by PCA__\n",
    ">\n",
    ">- Fit a logistic regression model on the train set\n",
    ">- Make predictions on the test set\n",
    ">- Calculate the *accuracy*, *precision* and *recall* using predictions and actual target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c40672",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
